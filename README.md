# ğŸš€ GPT-2 Comparison WebApp

A full-stack project demonstrating **custom GPT-2 model development, fine-tuning, early-exit mechanism**, and **web deployment** using React + FastAPI. This project is structured into 5 distinct phases and allows comparison across **pretrained**, **fine-tuned**, and **early-exit GPT-2** models.

---

## ğŸ“ Project Structure (Phases)

GPT2/
â”œâ”€â”€ Phase1/ # GPT-2 model implementation (from scratch)
â”œâ”€â”€ Phase2/ # Fine-tuning on WikiText-2 & FineWeb-EDU
â”œâ”€â”€ Phase3/ # Early-Exit Classifier after Transformer Block 9
â”œâ”€â”€ Phase4/ # Memory & Energy Profiling (per transformer layer)
â”œâ”€â”€ Phase5/ # Frontend (React) + Backend (FastAPI) web app
â”œâ”€â”€ app/ # Shared modules and utils (models.py, generate_text.py)
â”œâ”€â”€ venv/ # Python virtual environment (ignored by git)
â””â”€â”€ README.md

yaml
Copy
Edit

---

## ğŸ§  Phase Breakdown

### âœ… Phase 1 â€“ GPT-2 From Scratch (PyTorch)
- Implemented self-attention, transformer blocks, causal masking, and config setup.
- No HuggingFace libraries used â€“ **full custom GPT-2 model.**

### âœ… Phase 2 â€“ Fine-tuning GPT-2
- Trained on:
  - ğŸ—‚ï¸ WikiText-2
  - ğŸ§  FineWeb-EDU (10B token sample)
- Used gradient accumulation + learning rate scheduler (CosineAnnealingLR)
- Output checkpoint: `checkpoints/fine_tuned_gpt2_wikitext.pt`

### âœ… Phase 3 â€“ Early Exit Mechanism
- Introduced a classifier after Transformer Block 9
- Predicts whether model should exit early or continue full generation
- Output: `checkpoints/early_exit_classifier.pt`

### âœ… Phase 4 â€“ Profiling Memory & Energy
- Measured:
  - Memory usage per transformer layer
  - Compute time per layer (used as proxy for energy)
- Logs saved in `/Phase4/profiling_logs/`

### âœ… Phase 5 â€“ Web Interface (React + FastAPI)
- Users can input a prompt and choose model:
  - Pretrained
  - Fine-tuned
  - Early Exit
- Frontend: React + TailwindCSS + Vite
- Backend: FastAPI + CORS + prompt handler (`server.py`)
- Output shown instantly on frontend

---

## ğŸŒ Demo UI Preview

![UI Screenshot](./Phase5/screenshots/ui_preview.png) <!-- Replace with actual screenshot path -->

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Backend (FastAPI)
```bash
cd Phase5/backend
source ../../venv/bin/activate
uvicorn server:app --reload
2ï¸âƒ£ Frontend (React)
bash
Copy
Edit
cd Phase5/frontend
npm install       # first time only
npm run dev       # launches frontend at http://localhost:5174
ğŸ§ª API Endpoint
POST /generate

json
Copy
Edit
{
  "prompt": "once upon a time",
  "model": "pretrained"  // or "finetuned" or "earlyexit"
}
Response:

json
Copy
Edit
{
  "prompt": "once upon a time",
  "generated": "[Finetuned] response for: once upon a time"
}
ğŸ“Š Results
Model	Accuracy (HellaSwag)	Memory (MB/layer)	Energy Proxy (ms/layer)
Pretrained	43.2%	~82 MB	10.5 ms
Fine-tuned	65.1%	~82 MB	10.6 ms
Early Exit	61.4%	~82 MB	6.2 ms (with exit)

ğŸ› ï¸ Tech Stack
Area	Stack
Model	PyTorch, Tokenizer, Tiktoken
Training	WikiText-2, FineWeb-EDU
Profiling	Python timers, memory utils
Backend	FastAPI, Pydantic, Uvicorn
Frontend	React, Vite, TailwindCSS
Dev Tools	VS Code, GitHub, Mac MPS Backend

ğŸ“Œ TODO / Improvements
 Add streaming text generation

 Export profiling to CSV

 Compare more models (GPT-Neo, LLaMA-2, etc.)

 Deploy full-stack to HuggingFace Spaces or Render

ğŸ‘¨â€ğŸ’» Authors
 Nitish Behl â€“ GitHub
 Nitesh Malhotra 
